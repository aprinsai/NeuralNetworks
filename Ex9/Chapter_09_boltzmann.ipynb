{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9\n",
    "## Botzmann machines\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. Implement a Boltzmann machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import scipy.misc as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (unrestricted) Boltzmann machine is like a Hopfield network with stochastic state updates. The degree of randomness allows for investiations both in Hopfield networks and in how real associative memory may work (biological neurons are stochastic, too; and there are ideas that they make use of the irreducible noise). \n",
    "\n",
    "### Exercise 1: Flip-flop  (2 points)\n",
    "\n",
    "This is the **Boltzmann distribution** for a state $\\mathbf{x}$:\n",
    "\n",
    "$p(\\mathbf{x}) = \\frac{1}{Z} \\exp( \\frac{-E(\\mathbf{x})}{T} ) $\n",
    "\n",
    "with the state sum $Z$: \n",
    "\n",
    "$Z = \\sum_{\\mathbf{x}}  \\exp(\\frac{-E(\\mathbf{x})}{T})$\n",
    "\n",
    "\n",
    "Use it to derive the stable probability distribution of the **flip-flop** (2 units with bias $0.5$, connected with the identical weight $w_{12} = w_{21} = âˆ’1$). \n",
    "\n",
    "Assume the temperature $T=1$. Start by first computing $E(\\mathbf{x})$ for each possible state of the Hopfield network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "$ E(\\textrm{x}) = -\\frac{1}{2}\\textrm{x}^T\\textrm{W}\\textrm{x} - \\textrm{x}^T\\textrm{b} \\\\\n",
    "E(\\textrm{x}) = - \\frac{1}{2}\\textrm{x}^T\\left(\\begin{array}{cc} \n",
    "0 & -1\\\\ \n",
    "-1 & 0\n",
    "\\end{array}\\right)\\textrm{x} - \\textrm{x}^T\\textrm{(0.5, 0.5)} \\\\ \n",
    "E(\\textrm{[0,0]}) = 0\\\\\n",
    "E(\\textrm{[0,1]}) = -0.5\\\\\n",
    "E(\\textrm{[1,0]}) = -0.5\\\\\n",
    "E(\\textrm{[1,1]}) = 0 \\\\\\\\\n",
    "Z = \\sum_{\\mathbf{x}}  \\exp(-E(\\mathbf{x})) \\\\\n",
    "Z = \\exp(0) + \\exp(0.5) + \\exp(0.5) + \\exp(0)\\\\\n",
    "Z = 1 + 1.65 + 1.65 + 1 = 5.30 \\\\\\\\\n",
    "P = \\left(\\begin{array}{c} \n",
    "0.19 \\\\ \n",
    "0.31 \\\\\n",
    "0.31\\\\\n",
    "0.19\n",
    "\\end{array}\\right)\n",
    "\\textrm{Is this stable tho?}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement your own Boltzmann machine. As in the Hopfield network assignment, we will use an image and its mirrored version as the patterns the Boltzmann machine should learn. \n",
    "\n",
    "Note that we use the binary representation here, not the bipolar one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source image\n",
    "f = urllib2.urlopen(\"https://homepages.cae.wisc.edu/~ece533/images/watch.png\")\n",
    "\n",
    "# Read the image\n",
    "x1 = mpimg.imread(f)\n",
    "\n",
    "# Make binary\n",
    "x1 = np.mean(sp.imresize(x1,10),2)\n",
    "x1[x1 < np.mean(x1.flatten())] = 0   # Black\n",
    "x1[x1 >= np.mean(x1.flatten())] = 1  # White\n",
    "x1.astype(\"int32\")\n",
    "\n",
    "# Make duplicate but mirrored second image\n",
    "x2 = np.fliplr(x1)\n",
    "\n",
    "# Flatten images and put them into one matrix\n",
    "sz = x1.shape\n",
    "X = np.stack((x1.flatten(), x2.flatten()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xb67b128>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADeCAYAAAA+XgQvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAERxJREFUeJzt3b+rLttZAOC1YgJRrIQg2IjYWGjj3insgoiQQuxSiIhF\n/gexijZptFAEEUQkkMpSECwsAkIgcDbYCMEfRRqb6C0UBRUci3v2zXfPmTM/18y8a97ngQPJ3Xt/\ns2bWzPreedc7a+owDAUAIKvPXN0AAIArCYYAgNQEQwBAaoIhACA1wRAAkJpgCABITTAEAKQmGGJW\nrfUrtdZv11r/q9b6ravbA7BGrfX3a63/WGv9j1rrd2utv3F1m4jls1c3gC58VEr5g1LKz5RSfvHi\ntgCs9Z+llF8ppfxDKeWLpZS/rrX+0zAM3762WUQhM0QppZRa60/XWj+qtf782///E7XW79davzQM\nw98Mw/AXpZR/ubiZAKNmxrCvDcPw3WEY/m8Yhu+UUv62lPIL17aYSARDlFJKGYbhn0spv1VK+Wat\n9UdKKX9eSvnGMAzfurRhAAssHcNqrT9cPs4O/f3pjSSs6t1kPKq1/mUp5adKKUMp5YvDMPz3w8++\nWkr59WEYvnRR8wAmTY1hb3/+jVLKj5dSvjz4AuQtmSHe9aellJ8tpfzRu4MIQAc+OIbVWn/v7c++\nIhDikWCIT9Raf7R8XCj9Z6WU36m1/tjFTQJYbGoMq7X+binly6WUXx6G4d8vaiJBCYZ49IellDfD\nMHy1lPJXpZQ/KaWUWusP1Vo/Xz5++vAztdbP11o/d2E7AcZ8aAz77VLKr5VSfmkYhn+7sH0EpWaI\nUkoptdZfLaX8cSnl54Zh+OjtHdbflVK+Vkr5XPm4GPHRN4Zh+M1zWwkwbmYM+2Yp5X9KKf/78Cdf\nH4bh6+e3lIgEQwBAaqbJAIDUBEMAQGqCIQAgNcEQAJDaqhe11lpVW0MywzDUq9vQgvELUvrXYRi+\nMPdLMkMAwF19b8kvCYYAgNQEQwBAaoIhACA1wRAAkJpgCABITTAEAKQmGAIAUhMMAQCpCYYAgNQE\nQwBAaoIhACA1wRAAkJpgCABITTAEAKQmGAIAUhMMAQCpCYYAgNQEQwBAaoIhACA1wRAAkJpgCABI\nTTAEAKQmGAIAUvvs1Q2gL8MwfPK/a60XtgTgfcYotpAZAgBSEwwBAKkJhgCA1ARDAEBqCqjZ7LVQ\nUZEicLXHwmlYS2YIAEhNMEQqwzC4g4QZrhOyEQwBAKkJhgCA1FYFQ09PT1KnydVa3/sHcDVjE4/W\nTvXKDAEAqW16tN67XwCASPbMXMkMAQCpCYYAgNR2r0BtFWKiU/QP2yiJoActxniZIQAgtWbvJnMH\nQVSP56MsESxnLCeq1mO5zBAAkJpgCABIrdk02SNF1fc1Nx0ave+jtgsiiX6dTI0zSjbu7ahSB5kh\nACC1QzJDr0To9zPXj68/H4veI5wD0TNXEEGk62TtWBKhzbR1xoMvMkMAQGqCIQAgtU3TZFvWbYmU\ndmXc1j4amw4dO0eumja1thBsE+mabV0s7TspvrVj95415WSGAIDUNmWG9txpK6qOoWW2ZCwaf/xv\nU0XVZ7ACNWwTYYw+egmPqA97ZNUqvlhLZggASE0wBACkdug6Q3MUsJ1jaeqwRQFi65R2i2mtucLL\nFpzDXOmM6d+jrsW53ztqnJmavjd1dr6rSxhkhgCA1C7NDL1SVN3OFdH1lsdgz7Rnu959RA/Gshwt\nrsHW12yrz4s8zrHc1dmgRzJDAEBqgiEAILUQ02SPFFWvFynV+CpCm854aezY+XpUwTo5bDl/Wp9z\nU9dMj9PdRzFlvk3EvpQZAgBSC5cZeiXiHndkRL0kKxd1Fek971PbY2y7e4pZjyqE3er5+fmU7Zzh\n6empvHnz5pRtLS28b/UOrrHPO+ocWXsuH9mWMUu/O84oDvfd9QMRs0GPZIYAgNQEQwBAaqumyV5e\nXt5L+0VPfd1FxDTzHmunfub2/8zVcbdMRez5PddYf/acr63O9T3F0nt+b+3vnzkNfPa15IGgc+05\nzjJDAEBqdU2k/Pz8PJxVgDgmW3QdISOw9nHaqx+/nTtHIhzTyD5wl36LC6/W+l7nOx+mRb2eehuX\nHvkeO1et9WUYhtknQWSGAIDUBEMAQGqnrTPUIk1p7aHztSiaPDNFvXStj6vWQIlgqj8yXGNz/R1p\nSuVMc9dEpCmxq184y7TW67id0X8yQwBAaqdlho5a7fOOd69nrhw7t92Wx/fKlZSjbDeCNY9SZ1yB\nOuu5EWG/p8abFmPRljHOd9dyZ6zqfRSZIQAgNcEQAJBa2Be10t6WlOPav1mb+j1j6ixC+h96dOS1\nc9S01xWfQf9khgCA1LrPDB1d9HuFnh/tnWrz0n7J/Ng73NmWsfluY0Dv30+v7tYvMkMAQGqCIQAg\nte6nycb0vopupPTj1PFruTr13PYjHRNgnaXjcIvrvOWYdYSe1xmKcPyOIjMEAKS2KjP08vLyXjQb\nPVLsJQq/6hHWMwrQl74La2r7iqqhL61Wdm6ZHZ4bR87MKvU0g9HLmLvnOMoMAQCpCYYAgNRWBUNP\nT09lGIZP/etF1DZHbtOVL1Gd2n6t9dK08uv2o6e2ySfCuTm1/ehjy1Uitylau6a8G5+sabvMEACQ\n2i0frZ9zdVF1b5F2hO0fWVS9tkDz6mMCH7Ll3GxRoNyqWLqFnq/PCEXVPR+/PWSGAIDUBEMAQGph\np8nOWFcmQkryDFenPVtsf66v9qT6rz4+cKWjpsdaXVem2I53xn5HXytOZggASC1sZihi5LjX0e/d\nab2ts7W4y4x+9wG925JFj/6+sLXOGIevftCntej9LDMEAKTWLDPkjnyeN7BPG7sTmnqf2VHvUPvQ\ndiPqsc1302MfHNnmpddsL8fqKnfJCB2l9bkkMwQApCYYAgBSazZN1nvKc6r9kdKVvR/nlo583P6q\nv13L+XC9s/ugxbm5Z7tjsp2HPe5vj22e0np/ZIYAgNTCPlofSZbFGaNoHfHvKbTb8s6lu92BEcuZ\nWezWn+faOJfjvZzMEACQmmAIAEjNNNlKV68Kap2O4zie9M45fL0IY7TzYD2ZIQAgNZmht9Y+rrqn\nqHrpnUOEO4x39+2qx4hbtSHiKuAR+plrRO37Fpnv1tduizZEGEePKoCPvgxIdDJDAEBqgiEAIDXT\nZO84I229ZSouii1p3LH9WJpCHytYv1tq9y77wXp37Pu102NLf7/nNd7OKL9YqufjeCSZIQAgNZmh\nCREzEGe35d3tbSmKXHonsvR47zkGkfoSotqzhMjSv536eatx5orrPeoYIyM0TWYIAEhNMAQApJZ6\nmkzacL2xAr+5ovMWaylFTT2fIeJ0Lctl7b+l092tX6Sc7Ti3EHW9qzPJDAEAqaXODG0RKZt0xh3n\n2hVc96zk3drVdzit7rbO2I+s2Ysz9rvFZ191537GI+57Vk5e+3tnjJURVrl+le163kNmCABITTAE\nAKRmmmylPetv7BFpeu7V1LFoVRQZMc3be9G3dPoPTJ3DkY7J0rbc5dqZ+9sxUfY3wrkU5Vj0RGYI\nAEitq8zQnuxIpEg5+t3bo9bFn1OfG6l/p9oSvc/m9N7+I9zlmCx9t1WEgv6pguNWmfDe+jXiDEAp\nscbmo8gMAQCpCYYAgNROmyZrsQbD0hTw1PYfP+eqtuxxdspxSb9tSXOPfe5VxeljekntwodEP4f3\nTI9tLb4+ez2pSN8NrQvWW25rrg1nnMsyQwBAaqcXUB+5Km+EjMJaSyPfXt7Z06IPot/RAtv0mDHY\nM5ZuWRKhN2c+vHIkmSEAIDXBEACQ2mnTZGcUx/Y8vdLT2kMtjJ0PV+/3lnPz6jYvFek496LHY9bj\nOTx2nHueNloq+j5efV6c3QaZIQAgtdMLqCNEm9lMFZu3KNzes8J0hPNhzx3aVYWca7cX4Tj3Zs8x\nOzur1Ms5PGbLGPTa5tbvCYy+5EcGV52HMkMAQGqCIQAgta5e1BpV6zTqGdNJR6UiTcfsMzVl4dj2\nY66vrp6a6t1RU1dH9YcHh+KTGQIAUguRGZorgotaeHumtXeSZ670enabWjjyTm1pX00Va3Jvex5q\nOCqrFGkpgaNWeG791oOl281my0M1V59zMkMAQGqCIQAgtRDTZI96LiB9bN9R6dGzpwzPPOZTfd8q\nnXpG2nqquNOUGB8SqUSgxbTS0vKHM/Yn+lpPa0UfO+a+CyNOH8oMAQCphcgMLb0jYlyPBeZT/dt6\nf844lyIVApJD9PN6aab8buMX46KPkTJDAEBqgiEAILUQ02RzIqbUWjlqJdWlnxd9xdW79f2eKYGe\nXv4Z0cvLS6m1hn2p7t2O96O149zc70Uft9a688tge9k3mSEAILVwmaGpx/B6yhIsjYbH9u3MCDpi\ntB69WHrMkefmUcWnEfv+DK2yDmtX2T3y/YJnF1OPtWGJVm2+w7m75Vj09B34qpfvdJkhACA1wRAA\nkFqIabK59Ovrz6OvUzBmy1obWddd6mV67MgXZLb4mwznypGOmr45sl+uGjP2THf0Uljbwp7+6eW7\n7lGP3+kyQwBAaiEyQ3Pucgex9O5gbaFi78fl1dqC1Egitol7irhic8/Xbistxuir+/EIveyTzBAA\nkJpgCABILcQ02dI0Wi/ptjVaTJ3NHZeeU9MR2z5W9HfVOlGt3GUqeq2e9zv6NFSktrTQev2uO36f\nvZrbt4j7LjMEAKQWIjPEp7UutO7lkfWjHJk56+X4zGWuetmP1qb2O3q2r3Wb7pxhfnXVshgRMyF8\nmswQAJCaYAgASK37abJIK1ge6ag1irZsd8k2I4nevqX2TNtsWeF8GIby/Py8ajuRPT09lTdv3pyy\nJk70KbYxvbTzQ44e/02JjbvLd7DMEACQWveZobE7sJ6j0zX2ZIuWfh5xtLpzX/o5vWcK3vXy8vLB\nfWq9r3c7dnfWKgOYzd2+b2WGAIDUBEMAQGq3DIZqrZ/8y2YYhvf+LfV43Nb8o70t/bi2v5d83tPT\nU7PPu9rT01Pz47OmX7Zckyx39PilH+/93XrLYAgAYKnuC6iZN3UHM/eY8R3vACLb84j32kL51ssw\n9GLu/F76vrK1/fK43Z7fidazpX1PPjJDAEBqgiEAILVbTZNJca43d8yypvPX7vfSc6/V1MvSNqxd\nd+rO/Ty1ztCjpS9vXfL7W35vbnt7Pjv79Tz335h252MmMwQApHarzBBtZLtrHDO1uurYu3iOPGat\nC3rZbk92Z8vf7sn4tH7fWs8USzNHZggASE0wBACkZposkbulyM+YBohe8Ex8RxVkz/3NGedNz1Nx\nXmDNI5khACA1maHO9HLXNab1ytZjxaJn36kuXc25l4zPu8fv+fn5opa09/T0VN68efOp/3Z1H2zJ\nTuxZNXuPtSvZt9pWL300RlapHzJDAEBqgiEAIDXTZBe7OgW8x560eeu0fovP2TLFFn0157Vp+ght\nPsrYCtS9HJ9Whfxnr6C91tL+OHPKbo+1bTGtdh2ZIQAgNZmhBiLdiUSx5w7vqmLM6P24pag0+j5d\n7ajjE6kA+NGeTMWR12WL37sDxdrXkRkCAFITDAEAqaWeJouUvm7ljPVGXk29zHTs91qJVLjd2tS0\nYdQ2876lLwaN2qdnFDK3Li4+81hGHEdatSXrdJvMEACQWveZoUiReSRbCh+33u0ceUeyNPu0Z1tX\nPzL8uH3n830tzVpu+dsWzrjGIj3av3a8y5IxyVrELTMEAKQmGAIAUgs7TWa6oJ09LxM92lxatUXa\ndeyFrmPbuPPLJolv6hy5ar2fM4pyr7pOxqbulha+M67nKUeZIQAgtbomQqu1fr+U8r3jmgME85PD\nMHzh6ka0YPyClBaNYauCIQCAuzFNBgCkJhgCAFITDAEAqQmGAIDUBEMAQGqCIQAgNcEQAJCaYAgA\nSE0wBACk9v+bQ8IHz+YMKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9f280f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(X[:, 0].reshape(sz), cmap=\"gray\")\n",
    "ax[0].set_xticks([], [])\n",
    "ax[0].set_yticks([], [])\n",
    "ax[0].set_title(\"x1\")\n",
    "\n",
    "ax[1].imshow(X[:, 1].reshape(sz), cmap=\"gray\")\n",
    "ax[1].set_xticks([], [])\n",
    "ax[1].set_yticks([], [])\n",
    "ax[1].set_title(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: The sigmoid (1 point)\n",
    "Write a function `sigmoid(x)` that computes the *sigmoid activation function* $\\sigma(x)=\\frac{1}{1+exp(-x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    Y = 1/(1+np.exp(-x))\n",
    "    print Y\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Probabilities (1 point)\n",
    "\n",
    "Write a function `compute_probability(w, x, b, T)` that returns the probability that the state of node $x_i$ is set to $1$ ($0$ instead): \n",
    "\n",
    "$P(x_i=1)=\\sigma(\\frac{1}{T}(\\mathbf{w}_i^{\\top}\\mathbf{x}+b_i))$\n",
    "\n",
    "where $T$ is the temperature, $\\mathbf{w}_i$ the weights and $b_i$ the bias of node $i$; and $\\mathbf{x}$ the current state vector. $\\sigma$ is the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_probability(w, x, b, T):\n",
    "    Y = sigmoid((1/T)*(np.dot(w.T,x) + b)) #not sure if we need to index x, w and b ? What is x, w? Vector or matrix?\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gibbs sampling  (2 points)\n",
    "Write a function `gibbs_sampling(w, b, temperature=1.0, n_gibbs=20, n_burnin=10)` that approximates the model distribution for training a Boltzmann machine via Gibbs sampling. The Boltzmann machine is given by the current weights `w` and biases `b`.\n",
    "\n",
    "1. Create an array `X` for saving the node states for each time step. Then initialize a random initial node state vector $\\mathbf{x}^{(1)}$ and save it in `X[:,0]`. These are the node states for $t=1$.\n",
    "1. For $t=2, ..., N$, where $N$ is the number of Gibbs sampling steps `n_gibbs`:\n",
    "    1. Compute the probability $P(x^{t+1}_i = 1)$ for all node states $x^{t}_i$ in `X[:,t]` (the previous state vector). \n",
    "    1. Determine the new binary state $x^{t+1}_i$ by drawing a random number between 0 and 1 using `np.random.rand()`, and comparing it with $P(x^{t+1}_i = 1)$. That is, depending on the probability for $1$, it should be set to $1$ or $0$ at time `t+1`. Save the new state in `X[i,t+1]`. \n",
    "1. Return `X` without the burn-in phase, that is without the first `n_burnin` samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling(w, b, temperature=1.0, n_gibbs=20, n_burnin=10):\n",
    "    n_nodes = w.shape[0]\n",
    "    \n",
    "    # Initialize states\n",
    "    X = np.zeros((n_nodes, n_gibbs))\n",
    "                 \n",
    "    # Initialize first state vector\n",
    "    X[:,0] = np.random.randint(2, size=n_nodes)             \n",
    "    # Loop over Gibbs samples\n",
    "    for n in range(n_gibbs):             \n",
    "        # Loop over nodes\n",
    "        for i in range(X[:,n]):         \n",
    "            # Compute probability for state 1\n",
    "            P = compute_probability(w, X[i,n], b, temperature)     \n",
    "            # Sample whether it should change to 1\n",
    "            if (np.random.rand() < P):\n",
    "                X[i,n+1] = 1\n",
    "    # Discard burn-in\n",
    "    X = X[:, n_burnin:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Expectations  (1 point)\n",
    "Write a function `compute_expectations(X)` that computes the expectation (~mean over patterns / samples) of the partial derivatives for the weights and bias terms, given as: \n",
    "\n",
    "$$\\frac{\\partial J(x)}{\\partial w_{ij}}=-x_ix_j$$\n",
    "$$\\frac{\\partial J(x)}{\\partial b_i}=-x_i$$\n",
    "\n",
    "When running training you will compute these under the empirical and the model distribution (given as input `X`, which contains multiple patterns / sampling steps respectively). Note that if you use the dot product to compute $\\frac{\\partial J(x)}{\\partial w_{ij}}$ you need to divide by the number of patterns / samples `N` in `X`. Check for yourself (on paper, with a small example matrix `X` with 2 patterns) why this is so when you make use of the dot product to compute all $- x_i x_j$ at once. It will also help you arranging the dot product correctly. \n",
    "\n",
    "Hint: $I$ is the number of nodes. Then the partial derivative for the weight update `dw` should have the shape $(I, I)$. \n",
    "You also need to set the diagonal is set to the correct values for Boltzmann machines. You can complete this exercise in 3 lines of code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0, -5],\n",
      "       [-8,  0]]), array([[-1, -2],\n",
      "       [-3, -4]]))\n"
     ]
    }
   ],
   "source": [
    "# Compute expectations\n",
    "def compute_expectations(X):\n",
    "    I, N = X.shape\n",
    "    dw = np.dot(-X, X)/N\n",
    "    np.fill_diagonal(dw, 0)\n",
    "    return dw, -X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Boltzmann training  (1 point)\n",
    "Now, with the components you wrote above, you can fill in the missing part in `boltzmann_train`. What is missing is updating the weights `w` and biases `b` with the information gathered from the empirical and model distribution, and the learning rate `eta`. Add the two lines performing this step below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boltzmann_train(XE, temperature=1.0, eta=0.01, n_epochs=5, n_gibbs=20, n_burnin=10):\n",
    "    n_nodes, n_examples = XE.shape\n",
    "\n",
    "    # Initialize weights:\n",
    "    w = np.zeros((n_nodes, n_nodes))\n",
    "    \n",
    "    # Initialize biases:\n",
    "    b = np.zeros(n_nodes)\n",
    "    \n",
    "    # Compute expectations under the empirical distribution. As we compute this on \n",
    "    # the training patterns, we only need to do it once: \n",
    "    dE_dw, dE_db = compute_expectations(XE)     \n",
    "\n",
    "    # Loop over epochs: \n",
    "    for i_epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}.\".format(1 + i_epoch, n_epochs))\n",
    "        \n",
    "        # Gibbs sampling with the current model:\n",
    "        XM = gibbs_sampling(w, b, temperature, n_gibbs, n_burnin)\n",
    "\n",
    "        # Compute expectations under the model distribution: \n",
    "        dEM_dw, dEM_db = compute_expectations(XM)\n",
    "\n",
    "        # Update weights and biases\n",
    "            ### add your code here ###\n",
    "    \n",
    "    # Force symmetry\n",
    "    w = (w + w.T) / 2\n",
    "    \n",
    "    print(\"Training done.\")\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Dreaming (1 point)\n",
    "\n",
    "Write a function `boltzmann_dream(w, b, temperature=1.0, n_epochs=20)` that you can use to sample images from a previously trained Boltzmann machine (given as `w` and `b`) that is left running on its own for `n_epochs`. This boils down to performing Gibbs sampling starting with a random state vector. Use your previously implemented functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boltzmann dreaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 (1 point)\n",
    "\n",
    "**1.** Train your Boltzmann machine with `boltzmann_train` on the two images saved in `X` (the empirical distribution `XE`). Use $T=1$, 20 Gibbs sampling steps, a burn-in phase of 10, 3 training epochs, and a learning rate of $0.01$. Then let your trained Boltzmann machine dream for 20 epochs via `boltzmann_dream`. Plot the last 5 states as images. \n",
    "\n",
    "**2.** Now, change the temperature to $T=5$. Again, train the Boltzmann machine, let it run freely for 20 epochs and then plot the last 5 states it creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Boltzmann machine\n",
    "\n",
    "# Test Boltzmann machine\n",
    "\n",
    "# Plot last 5 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Boltzmann machine\n",
    "\n",
    "# Test Boltzmann machine\n",
    "\n",
    "# Plot last 5 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
