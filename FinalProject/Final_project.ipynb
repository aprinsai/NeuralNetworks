{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project: Competition\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. Build your own state-of-the-art artificial neural network\n",
    "1. Work on a real-world problem definition\n",
    "1. Work in a machine learning competition framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final assignment is a neural network competition. Your task is to classify categories of clothing on the Fashion-MNIST data set, which has recently been proposed as a more challenging alternative to the standard MNIST dataset. For a more detailed description, some illustrations of the categories, reasons why Fashion-MNIST might be a better benchmark dataset than the classic MNIST dataset, and more, you can check [their GitHub](https://github.com/zalandoresearch/fashion-mnist). \n",
    "\n",
    "The competition is set up as an InClass competition [on Kaggle](https://www.kaggle.com/t/de281a20013a41a3ab724ec020848e23). If you want to continue following the machine learning direction, Kaggle is a great platform for training skills on real-world problems. Via our Kaggle InClass competition, you will be provided with a training and a validation dataset for your own (local) use, and with the features of the competition test set, for which you will generate predictions top be submitted to Kaggle. \n",
    "\n",
    "In this notebook, we provide a code basis which loads the dataset, trains a simple model, and writes the competition test set predictions into the file `test_set_predictions.csv`. This is the file you will create with your own model, and that you should submit to the competition website. \n",
    "\n",
    "The test dataset that we provide you with, contains approximately 42% of the actual total test data we have. The leaderboard will, during the competition, show the performance of your model on this 42% split. Please note, that the final results will be based on the other 58%. This means that overfitting the provided test dataset using the leaderboard will highly likely result in a poor performance on the final evaluation. This also means that the final standings may be different. The final evaluation will be performed after the deadline. \n",
    "\n",
    "Based on the final evaluation, the Top 6 project groups will earn the following bonus on their assignment grades (and eternal fame): \n",
    "1. **Top 1 and 2** will receive +1.0 on their overall assignment grade. \n",
    "1. **Top 3 and 4** will receive +0.5 on their overall assignment grade. \n",
    "1. **Top 5 and 6** will receive +0.3 on their overall assignment grade. \n",
    "*(Note that the maximum average assignment grade you can reach is 10)*\n",
    "\n",
    "If you think that it is necessary you can implement and run your model in a normal Python script, however you need to paste the finalized code including the exercises in this notebook **inside this notebook**. Thus, this final assignment is nothing more than the other assignments: you submit a single notebook file to Blackboard before the deadline, and the assignment counts as the last assignment in the list of all assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (1 point)\n",
    "\n",
    "1. **Create an account** for your group on Kaggle. Set your `Display Name` to your group number - using the format: `Group07`, `Group42`. You can also use your existing Kaggle account. Just temporarily change your `Display Name` to the group name. \n",
    "\n",
    "1. **Join our InClass competition** [on the Kaggle website](https://www.kaggle.com/t/de281a20013a41a3ab724ec020848e23). \n",
    "\n",
    "1. **Download the dataset** (python pickle) and the example submission from the 'data' tab at the competition webstite. \n",
    "\n",
    "1. After doing all the above, **submit the example submission** file `sampleSubmission.csv`. You do so by clicking 'Submit Predictions'. Here, you can upload the submission file, and also annotate it with versioning information or so.\n",
    "\n",
    "You will receive the point for this exercise if we see your group name in the leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (1.5 points)\n",
    "\n",
    "**Implement your neural network** within python 2.0 using the chainer library, just like all other assignments. Below, we have provided some preliminary code from which you can start. For instance, you could improve or use: \n",
    "\n",
    "* the neural network in the model class (obviously)\n",
    "* the types of the [neural network layers](https://docs.chainer.org/en/stable/reference/links.html) (also check [this website to get some intuition](https://github.com/vdumoulin/conv_arithmetic))\n",
    "* the objective function\n",
    "* various types of preprocessing including [z-standardisation](http://scikit-learn.org/stable/modules/preprocessing.html) and data augmentation\n",
    "* weight initialization\n",
    "* prevention of overfitting via cross-validation\n",
    "* batch normalization\n",
    "* early stopping\n",
    "* regularization hooks (L1, L2)\n",
    "* dropout\n",
    "* and much more\n",
    "\n",
    "Most of these are small tricks that are easy to look up and add to your model. And most of them are implemented in state-of-the-art neural networks frameworks. Check [this page](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html) for a brief summary of many common tricks including  code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (1 point)\n",
    "\n",
    "Make an informed choice for your `optimizer`. A good guide on various optimizers can be found [here](http://ruder.io/optimizing-gradient-descent/). Not all, but most of them are implemented in chainer. **Explain** why you have chosen this particular optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (2 points)\n",
    "\n",
    "**Explain** your neural network implementation in a structured way. Clearly motivate your choices. For instance, motivate the architecture (number and type of layers, number of units, non-linearities, extensions like dropout, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (3.5 points)\n",
    "\n",
    "**Evaluate** the performance of your model on the **local test set**. \n",
    "\n",
    "As a minimum, you should: \n",
    "1. Plot the training vs. test accurracy across training epochs and embed the resulting plot here. \n",
    "1. Plot some of the weights that your model has learned in the first layer.\n",
    "1. Show 3 examples from the local test set that the model was unable to classify correctly.  (Check for which images real labels and predicted labels do not match. Plot the corresponding images.)\n",
    "1. (Text question) Did your model perform well or not? Based on your observations, which architectural choices do you think have influenced performance most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (1 point)\n",
    "\n",
    "**Submit** `test_set_predictions.txt` to the [Kaggle competition website](https://www.kaggle.com/t/de281a20013a41a3ab724ec020848e23). You can submit the file twice a day to check your position in the leaderboard, and to improve your position. You will receive this point if we see a reasonable submission attempt online. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and dataset\n",
    "\n",
    "Note, we have `train_features` and `train_labels` which you can use to train your model, `local_test_features` and `local_test_labels` which you can use to validate your model locally, and `kaggle_test_features`, which is the dataset for which you need to generate predictions and submit to the online Kaggle competition to obtain a performance for the leaderboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import csv, pickle\n",
    "\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.datasets as D\n",
    "from chainer.training import extensions\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xab77e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaad44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('competition_data.pkl', 'rb') as handle:\n",
    "    competition_data = pickle.load(handle)\n",
    "\n",
    "# Training data\n",
    "train_features = competition_data['train_features'].astype('float32')\n",
    "train_labels = competition_data['label_train'].astype('int8')\n",
    "\n",
    "# Local test data\n",
    "local_test_features = competition_data['local_test_features'].astype('float32')\n",
    "local_test_labels = competition_data['local_test_labels'].astype('int8')\n",
    "\n",
    "# Kaggle feature data\n",
    "kaggle_test_features = competition_data['kaggle_test_features'].astype('float32')\n",
    "\n",
    "# Show data example\n",
    "plt.figure()\n",
    "plt.imshow(train_features[123].reshape([28,28]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add dimension???????\n",
    "train_features_new_dim = np.expand_dims(train_features, axis = 1)\n",
    "local_test_features_new_dim = np.expand_dims(local_test_features, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create chainer datasets\n",
    "train = D.TupleDataset(train_features_new_dim, train_labels)\n",
    "test = D.TupleDataset(local_test_features_new_dim, local_test_labels)\n",
    "\n",
    "# Create chainer iterators\n",
    "train_iter = iterators.SerialIterator(train, batch_size=100, shuffle=True)\n",
    "test_iter = iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model classes\n",
    "\n",
    "Note, you do not necessarily need to use the templates as provided here. Use them as a starting point instead. Logistic regression can be framed as a neural network, which we provide below. Note that SVMs and decision trees are not allowed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement the neural network for your competition here:\n",
    "class CompetitionNetwork(Chain):\n",
    "    def __init__(self, n_out, n_units):\n",
    "        super(CompetitionNetwork, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.conv1 = L.Convolution2D(None, n_units, stride = 2, ksize = 14)  # n_in -> n_units\n",
    "            self.conv2 = L.Convolution2D(None, n_units, stride = 2, ksize = 7)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "            self.norm = L.BatchNormalization(n_units)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.conv1(x))\n",
    "        hnorm1 = F.relu(self.norm(h1))\n",
    "        h2 = F.max_pooling_2d(hnorm1, ksize = 2)\n",
    "        hnorm2 = F.relu(self.norm(h2))\n",
    "        h3 = F.relu(self.conv2(x))\n",
    "        hnorm3 = F.relu(self.norm(h3))\n",
    "        h4 = F.max_pooling_2d(hnorm3, ksize = 2)\n",
    "        hnorm4 = F.relu(self.norm(h4))\n",
    "        y = self.l3(hnorm3)\n",
    "        return y\n",
    "    \n",
    "# Linear regression model for testing the competition process:\n",
    "class LinearRegression(Chain):\n",
    "    def __init__(self, n_out):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.lr = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = self.lr(x)\n",
    "        return y\n",
    "\n",
    "# The classifier setup, including the objective/loss function\n",
    "class Classifier(Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t) # The objective function\n",
    "        accuracy = F.accuracy(y, t)\n",
    "        report({'loss': loss, 'accuracy': accuracy}, self)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x):  \n",
    "        yclass = F.softmax( self.predictor(x) ).data.argmax(axis=1)\n",
    "        return yclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up optimizer and run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/accuracy  validation/main/accuracy  main/loss   validation/main/loss\n",
      "1           0.8226         0.8212                    0.529412    0.566017              \n",
      "2           0.866733       0.796                     0.375397    0.661278              \n",
      "3           0.878367       0.8398                    0.341937    0.516072              \n",
      "4           0.885683       0.8198                    0.319783    0.58293               \n",
      "5           0.890717       0.8374                    0.304798    0.553986              \n",
      "6           0.893567       0.8478                    0.293216    0.510965              \n",
      "7           0.89765        0.8524                    0.284152    0.501563              \n",
      "8           0.89945        0.8276                    0.276412    0.622357              \n",
      "9           0.902867       0.8516                    0.266223    0.513011              \n",
      "10          0.90485        0.8318                    0.262012    0.600254              \n",
      "11          0.908017       0.8536                    0.257082    0.526303              \n",
      "12          0.908483       0.8544                    0.251446    0.517249              \n",
      "13          0.90995        0.8126                    0.248991    0.696067              \n",
      "14          0.910917       0.8494                    0.244504    0.537914              \n",
      "15          0.913          0.8536                    0.24014     0.525973              \n",
      "16          0.912633       0.8428                    0.236128    0.605025              \n",
      "17          0.914617       0.8366                    0.233162    0.610217              \n",
      "18          0.916433       0.8442                    0.230448    0.619833              \n",
      "19          0.918433       0.8468                    0.225484    0.580345              \n",
      "20          0.917917       0.8466                    0.224996    0.620697              \n",
      "21          0.919467       0.858                     0.221815    0.576405              \n",
      "22          0.918167       0.8218                    0.219481    0.714667              \n",
      "23          0.920833       0.852                     0.218627    0.597292              \n",
      "24          0.921633       0.8468                    0.214541    0.627807              \n",
      "25          0.92205        0.8426                    0.212929    0.669463              \n",
      "26          0.921783       0.8456                    0.212144    0.655182              \n",
      "27          0.923567       0.834                     0.209145    0.710711              \n",
      "28          0.92495        0.8456                    0.206942    0.677118              \n",
      "29          0.925417       0.8338                    0.205649    0.733673              \n",
      "30          0.92565        0.8314                    0.204971    0.734857              \n",
      "31          0.925117       0.7866                    0.203885    0.983035              \n",
      "32          0.926833       0.8428                    0.202219    0.69682               \n",
      "33          0.926833       0.828                     0.200108    0.767035              \n",
      "34          0.9287         0.8438                    0.197639    0.717254              \n",
      "35          0.927617       0.8394                    0.197654    0.76497               \n",
      "36          0.927917       0.8368                    0.197116    0.757341              \n",
      "37          0.928383       0.8442                    0.194922    0.724228              \n",
      "38          0.928733       0.8344                    0.193931    0.764322              \n",
      "39          0.92945        0.8488                    0.19429     0.650737              \n",
      "40          0.9298         0.842                     0.191562    0.698056              \n",
      "41          0.93065        0.8348                    0.19009     0.763218              \n",
      "42          0.92995        0.8174                    0.191585    0.946573              \n",
      "43          0.930417       0.8356                    0.189167    0.771905              \n",
      "44          0.931133       0.8352                    0.186718    0.781801              \n",
      "45          0.931633       0.8408                    0.186877    0.748673              \n",
      "46          0.931483       0.838                     0.186566    0.766961              \n",
      "47          0.932633       0.8226                    0.184476    0.899905              \n",
      "48          0.932483       0.8352                    0.183349    0.801382              \n",
      "49          0.9332         0.8274                    0.184307    0.840611              \n",
      "50          0.933283       0.8428                    0.183496    0.748731              \n"
     ]
    }
   ],
   "source": [
    "# Setup model\n",
    "model = Classifier(CompetitionNetwork(n_out=10,n_units=16))   # replace this with CompetitionNetwork\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.SGD(lr=0.1)\n",
    "optimizer.setup(model)\n",
    "\n",
    "# Setup trainer\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (50, 'epoch'), out='competition_output')\n",
    "\n",
    "# Add local validation set\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "\n",
    "# Print/log results\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/accuracy\", \"validation/main/accuracy\",  \"main/loss\", \"validation/main/loss\"]))\n",
    "\n",
    "# Start training\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_test_features_new_dim = np.expand_dims(kaggle_test_features, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on competition data set and write submission file\n",
    "\n",
    "Note that the submission file is a simple CSV file which contains two columns: The item `ID` and the predicted `class` label. With the code below, you can transform the vector of predictions into the required submission file. Submit this CSV file manually to the competition website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict labels of the kaggle test dataset using the trained model\n",
    "kaggle_test_predicted_labels = model.predict(kaggle_test_features_new_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the submission file\n",
    "f = open(\"kaggle_test_predicted_labels.csv\", \"w\")\n",
    "w = csv.writer(f, delimiter=',')\n",
    "w.writerow(['ID','class'])\n",
    "for item_id in xrange(kaggle_test_features.shape[0]):\n",
    "    w.writerow([item_id, kaggle_test_predicted_labels[item_id]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
