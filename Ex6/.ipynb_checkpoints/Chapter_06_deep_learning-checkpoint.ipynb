{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "## Deep learning\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "\n",
    "Learning goals:\n",
    "1. Get familiar with a state-of-the-art framework for deep learning\n",
    "1. Implement and run a convolutional neural network\n",
    "1. See what convolution does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named chainer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fe873a213502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named chainer"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import os, json\n",
    "import scipy.signal as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_output_folder = \"chainermlp/\"\n",
    "cnn_output_folder = \"chainercnn/\"\n",
    "\n",
    "def get_losses(output_folder):\n",
    "\n",
    "    with open(os.path.join(output_folder, \"log\")) as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    training_loss = map(lambda x: x[\"main/loss\"], data)\n",
    "    validation_loss = map(lambda x: x[\"validation/main/loss\"], data)\n",
    "    \n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When working on a state-of-the-art neural network project you would rely on one of the various neural network frameworks that are available, instead of implementing core functionality by yourself. In this exercise you will work on implementing an MLP and a convolutional neural network in the Chainer framework. At the moment we use this framework for most of the work in our group as it is flexible and has quite intuitive and pythonic syntax.  \n",
    "\n",
    "Due to the popularity of deep learning various [Neural Network Frameworks](http://chainer.readthedocs.io/en/latest/comparison.html) exist, and it is unclear whether one of them will \"win\". Among those mentioned, Chainer and PyTorch (which is quite popular right now) have very similar syntax, and in fact PyTorch started as a fork from Chainer, reimplementing core functionality such as autograd in C++. This makes it faster, but less versatile. Google is developing and promoting the TensorFlow framework, which is fast for statically defined neural network flows. \n",
    "\n",
    "\n",
    "In this assignment you will follow a Chainer tutorial for implementing a standard multi-layer perceptron and a convolutional neural network. You will first copy the tutorial code and then make several adaptations. For a few steps you should consult the documentation or example code to check out how to implement them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the tutorial at https://docs.chainer.org/en/stable/tutorial/basic.html. As you go along, transfer the code for the Multi-Layer Perceptron example on MNIST into the notebook (and try running it). Use the variant that makes use of the Trainer framework, i.e. you do not need to handle the gradients yourself. \n",
    "\n",
    "Change the MLP training code so that it is similar to the MLP we implemented in the MLP exercise, i.e. it should use: \n",
    "\n",
    "1. A single hidden layer of 16 units (that is, two weight layers)\n",
    "1. ReLU activation function\n",
    "1. Stochastic Gradient Descent with a learning rate of 0.001 as optimizer\n",
    "1. Training should do 50 iterations over the training set\n",
    "\n",
    "For **evaluating the model**, set up the `trainer` object as follows: \n",
    "1. When initializing the `trainer` object, set the output folder to be `mlp_output_folder`.\n",
    "1. Extend your `trainer` object with an `Evaluator` object that evaluates your `model` on the test set. \n",
    "1. Extend your `trainer` object with the `LogReport()` object. \n",
    "1. For printing the current model performance underneath the cell, extend the `trainer` object with a `PrintReport` object. Use the keys `\"epoch\"`, `\"main/accuracy\"`, `\"validation/main/accuracy\"`,  `\"main/loss\"` and `\"validation/main/loss\"`.\n",
    "1. For plotting the accuracy and losses over epochs, extend the `trainer` object with a `PlotReport` object. Use the same keys as for `PrintReport` except `\"epoch\"`. Set the file name to `mlp_plot.png`.\n",
    "\n",
    "Embed the generated `mlp_plot.png` underneath your code via `<img src='chainermlp/mlp_plot.png'>`. **Please additionally attach `mlp_plot.png` to your submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single example: (784,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST data set as 784-length vectors (default): \n",
    "train, test = datasets.get_mnist()\n",
    "print \"Shape of a single example:\", train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your MLP, including output\n",
    "    ### Add your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Embed `mlp_plot.png`: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The input layer weights that the MLP has learned can be found in `model.predictor.l1.W.data`. Plot them below (as we did in the MLP and autoencoder exercises). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the weights\n",
    "    ### Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Convolution with hand-made kernels (1 point)\n",
    "In convolutional neural networks we use a mathematical operator between two functions $f$ and $g$ called [*convolution*](https://en.wikipedia.org/wiki/Convolution). In terms of images, one could intuitively think of $f$ as an image, and $g$ as a filter kernel (i.e., a receptive field). For images we need to use 2D convolution: \n",
    "\n",
    "$$f(x, y) \\ast g(x, y) = \\sum^N_{i=-N}\\sum^N_{j=-N} f(x, y)g(x-i, y-j)$$\n",
    "\n",
    "Intuitively, the kernel $g$ is applied on all spatial locations of $f$. You could think of $g$ as having weights that represent how it singles out local input. Usually there are various of these filter kernels $g$, scanning the image for many possible features. \n",
    "\n",
    "In this exercise you will perform a simple convolution on an image. We first choose an image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11596b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = train[2][0].reshape((28, 28))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.imshow(image, cmap=\"gray\")\n",
    "ax.set_xticks([], []) ; ax.set_yticks([], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make four $3 \\times 3$ kernels. They should represent horizontal, vertical, and the two diagonal line feature detectors.\n",
    "1. Plot these kernels.\n",
    "1. Apply the kernels to the image, e.g. by using `convolved2d` from `scipy`.\n",
    "1. Plot the resulting feature activity map for each of the kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Create kernels\n",
    "kernel_template = [ [-1.0, -1.0, -1.0], \n",
    "                    [-1.0,  2.0, -1.0], \n",
    "                    [-1.0, -1.0, -1.0] ]   # example: point detector\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# 2. Plot kernels\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# 3. Convolve the image with the kernels\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# 4. Plot the activity maps\n",
    "    ### Add your code here. ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 4 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to train a convolutional neural network (CNN) next. Check [this project](https://github.com/vdumoulin/conv_arithmetic) to get intuition for the operations in convolutional neural network layers. \n",
    "\n",
    "\n",
    "1. Create `class CNN(Chain)` that should contain the new model. It will be similar to the `MLP` class, so you can start by copying the code of the MLP model class. \n",
    "1. The network should have one [2D-convolutional layer](https://docs.chainer.org/en/stable/reference/links.html). The first layer `conv1` should have 4x4 kernels, a stride of 1 and 10 output channels. As in the MLP, a second `Linear` layer should connect with the output units. \n",
    "1. Use ReLU activation functions on the convolutional layer activation.\n",
    "1. Add 2D max-pooling with a `(2,2)` pooling window after the convolutional layer. \n",
    "1. Copy the rest of the training code underneath and adapt it so that it uses your `CNN` model. Change the plot file name to `cnn_plot.png`. Run the model for 30 full iterations through the training set and embed `cnn_plot.png` as before. The output directory should be `cnn_output_folder` now. **Please additionally attach `cnn_plot.png` to your submission.**\n",
    "\n",
    "This will take a while to run. You can test the code by reducing the number of output channels of `conv1` first.  Usually you would train ConvNets on GPUs, which implement many of the linear algebra operations that are used here much more efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single example: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST data set in shape 1x28x28-length (the 2D shape is necessary for the convolutional layer): \n",
    "train, test = datasets.get_mnist(ndim=3)\n",
    "print \"Shape of a single example:\", train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your CNN, including output\n",
    "    ### Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed `cnn_plot.png`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (0.5 points)\n",
    "The input layer weights that the ConvNet has learned can be found in `model.predictor.conv1.W.data`. Plot them below (as we did in the MLP and autoencoder exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the weights\n",
    "    ### Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (2 points)\n",
    "1. Which of the two models performed better for classifying MNIST digits in this example? Compare speed of training and test set accuracy. \n",
    "\n",
    "1. The main difference of the models is the type of the first layer. What leads to the success of the first layer of the better-performing model? \n",
    "\n",
    "1. Explain ways in which the convolution operation and the resulting feature maps are biologically plausible (1 way) and biologically implausible (1 way). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution 6\n",
    "1. Write your answer here.\n",
    "1. Write your answer here.\n",
    "1. Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around further with your code for this assignment by replacing the MNIST dataset with a more interesting / difficult to solve data set. For instance, you could use: \n",
    "\n",
    "`chainer.datasets.get_cifar10()`  ([CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) - photos of 10 natural object classes)\n",
    "\n",
    "and \n",
    "\n",
    "`chainer.datasets.get_svhn()`  ([SVHN](http://ufldl.stanford.edu/housenumbers/) - house numbers from Google StreetView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
