{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7\n",
    "## Recurrent neural networks\n",
    "\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "\n",
    "Learning goals:\n",
    "1. Get familiar with recurrent hidden units\n",
    "1. Implement a simple recurrent neural network (Elman network)\n",
    "1. Implement an LSTM-based neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import TupleDataset\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a recurrent neural network with one input unit $x$, one sigmoid recurrent hidden unit $h$, and one linear output unit $y$. The values of $x$ are given for 3 time points in `x_t`. As this is a very small RNN, $W^i$, $W^h$ and $W^o$ are given as the scalar values `w_i`, `w_h` and `w_o` respectively. The hidden unit has an added bias `h_bias`. The hidden unit state is initialized with `0.0`. The only 'value-manipulating' activation function in this network is the sigmoid activation $\\sigma(\\cdot)$ on the hidden unit. \n",
    "\n",
    "1. Write down the forward pass of this network for a specific time point $t$. \n",
    "1. What is the value of the hidden state $h$ after processing the last input `x_t[2]`? \n",
    "1. What is the output `y` of the network after processing the last input `x_t[2]`? \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "h_t &=& \\\\ \n",
    "y_t &=&  \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "You can either compute the solution by hand (show clearly how you arrived there, 3 decimal points) or write code to find the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs over times 0, 1, 2:\n",
    "x_t = [9.0, 4.0, -2.0]\n",
    "\n",
    "# weights and bias terms: \n",
    "w_i = 0.5\n",
    "w_h = -1.0\n",
    "w_o = -0.7\n",
    "h_bias = -1.0\n",
    "y_bias = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code introduction\n",
    "\n",
    "We will apply two recurrent neural networks to learn a dynamic variant of the *adding problem*. First, run the next cell and check the output. \n",
    "\n",
    "There is a stream of inputs to the network, two at each time step. The first input unit will receive a series of decimal numbers in the interval $[-1,1]$. The second input unit will receive the numbers $0$, $-1$, and $1$. The target is the sum of the preceding two decimal numbers that came together with the number $1$ (called the marker, `x` in the generated output), and it should be produced whenever a marker has been seen. In the beginning until two of these markers have been seen, the input will stay 0. \n",
    "\n",
    "\n",
    "Below you will find several functions: \n",
    "1. `create_addition_data`: Generates sequential training data sets `X` and `T` for the dynamic *adding problem*, returns a `TupleDataset` for `chainer`.\n",
    "1. `SequentialIterator`: An `Iterator` determines in which order to traverse a training data set. If you have separate individual examples it is best to use a random order. As our data set is sequential, we need to go through it from beginning to end, which is what we implement in this custom `Iterator`. \n",
    "1. `train_model`: A function that takes your model definition and the generated data, and trains the parameters accordingly.\n",
    "4. `Regressor`: Structure for defining a regression problem. Calls a `predictor` (model doing regression) with an input and returns the error between its output and the desired output. \n",
    "\n",
    "Code example for generating your data and setting up the `SequentialIterator` on it:  \n",
    "\n",
    "`train_iter = SequentialIterator( create_addition_data(n_samples), batch_size=batch_size )`\n",
    "\n",
    "\n",
    "Code example for training a model:\n",
    "\n",
    "`model, train_loss, test_loss = train_model( train_iter, test_iter, n_epochs=n_epochs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data for the adding problem (x marks 1.0):\n",
      "Time= 0:   x: (+0.50 ,   )  |  t:  +0.00 \n",
      "Time= 1:   x: (-0.45 ,   )  |  t:  +0.00 \n",
      "Time= 2:   x: (-0.42 ,   )  |  t:  +0.00 \n",
      "Time= 3:   x: (+0.99 ,   )  |  t:  +0.00 \n",
      "Time= 4:   x: (-0.53 ,   )  |  t:  +0.00 \n",
      "Time= 5:   x: (+0.94 ,   )  |  t:  +0.00 \n",
      "Time= 6:   x: (+0.56 ,   )  |  t:  +0.00 \n",
      "Time= 7:   x: (+0.30 ,   )  |  t:  +0.00 \n",
      "Time= 8:   x: (+0.73 ,   )  |  t:  +0.00 \n",
      "Time= 9:   x: (-0.99 ,   )  |  t:  +0.00 \n",
      "Time=10:   x: (+0.83 ,   )  |  t:  +0.00 \n",
      "Time=11:   x: (+0.18 ,   )  |  t:  +0.00 \n",
      "Time=12:   x: (-0.36 ,   )  |  t:  +0.00 \n",
      "Time=13:   x: (+0.23 ,   )  |  t:  +0.00 \n",
      "Time=14:   x: (-0.32 , x )  |  t:  +0.00 \n",
      "Time=15:   x: (-0.63 ,   )  |  t:  +0.00 \n",
      "Time=16:   x: (-0.91 ,   )  |  t:  +0.00 \n",
      "Time=17:   x: (-0.86 ,   )  |  t:  +0.00 \n",
      "Time=18:   x: (-0.47 ,   )  |  t:  +0.00 \n",
      "Time=19:   x: (-0.47 ,   )  |  t:  +0.00 \n",
      "Time=20:   x: (-0.94 ,   )  |  t:  +0.00 \n",
      "Time=21:   x: (+0.78 ,   )  |  t:  +0.00 \n",
      "Time=22:   x: (+0.41 ,   )  |  t:  +0.00 \n",
      "Time=23:   x: (+0.31 ,   )  |  t:  +0.00 \n",
      "Time=24:   x: (+0.59 ,   )  |  t:  +0.00 \n",
      "Time=25:   x: (+0.17 ,   )  |  t:  +0.00 \n",
      "Time=26:   x: (-0.76 ,   )  |  t:  +0.00 \n",
      "Time=27:   x: (-0.98 ,   )  |  t:  +0.00 \n",
      "Time=28:   x: (-0.52 ,   )  |  t:  +0.00 \n",
      "Time=29:   x: (-0.29 ,   )  |  t:  +0.00 \n",
      "Time=30:   x: (-0.20 ,   )  |  t:  +0.00 \n",
      "Time=31:   x: (-0.30 ,   )  |  t:  +0.00 \n",
      "Time=32:   x: (+0.98 ,   )  |  t:  +0.00 \n",
      "Time=33:   x: (+0.76 ,   )  |  t:  +0.00 \n",
      "Time=34:   x: (-0.12 ,   )  |  t:  +0.00 \n",
      "Time=35:   x: (-0.22 ,   )  |  t:  +0.00 \n",
      "Time=36:   x: (+0.03 ,   )  |  t:  +0.00 \n",
      "Time=37:   x: (+0.80 ,   )  |  t:  +0.00 \n",
      "Time=38:   x: (-0.27 ,   )  |  t:  +0.00 \n",
      "Time=39:   x: (-0.58 ,   )  |  t:  +0.00 \n",
      "Time=40:   x: (+0.26 ,   )  |  t:  +0.00 \n",
      "Time=41:   x: (+0.48 ,   )  |  t:  +0.00 \n",
      "Time=42:   x: (-0.81 , x )  |  t:  -1.13 \n",
      "Time=43:   x: (+0.38 ,   )  |  t:  +0.00 \n",
      "Time=44:   x: (+0.10 ,   )  |  t:  +0.00 \n",
      "Time=45:   x: (+0.30 ,   )  |  t:  +0.00 \n",
      "Time=46:   x: (-0.64 ,   )  |  t:  +0.00 \n",
      "Time=47:   x: (+0.89 ,   )  |  t:  +0.00 \n",
      "Time=48:   x: (-0.94 ,   )  |  t:  +0.00 \n",
      "Time=49:   x: (-0.47 ,   )  |  t:  +0.00 \n",
      "Time=50:   x: (-0.95 ,   )  |  t:  +0.00 \n",
      "Time=51:   x: (-0.01 ,   )  |  t:  +0.00 \n",
      "Time=52:   x: (+0.49 ,   )  |  t:  +0.00 \n",
      "Time=53:   x: (-0.15 , x )  |  t:  -0.96 \n",
      "Time=54:   x: (-0.65 ,   )  |  t:  +0.00 \n",
      "Time=55:   x: (-0.98 ,   )  |  t:  +0.00 \n",
      "Time=56:   x: (+0.34 ,   )  |  t:  +0.00 \n",
      "Time=57:   x: (-0.73 ,   )  |  t:  +0.00 \n",
      "Time=58:   x: (+0.45 ,   )  |  t:  +0.00 \n",
      "Time=59:   x: (+0.97 ,   )  |  t:  +0.00 \n",
      "Time=60:   x: (-0.87 ,   )  |  t:  +0.00 \n",
      "Time=61:   x: (+0.74 ,   )  |  t:  +0.00 \n",
      "Time=62:   x: (-0.76 ,   )  |  t:  +0.00 \n",
      "Time=63:   x: (+0.16 ,   )  |  t:  +0.00 \n",
      "Time=64:   x: (+1.00 , x )  |  t:  +0.85 \n",
      "Time=65:   x: (-0.27 ,   )  |  t:  +0.00 \n",
      "Time=66:   x: (-0.15 ,   )  |  t:  +0.00 \n",
      "Time=67:   x: (+0.27 ,   )  |  t:  +0.00 \n",
      "Time=68:   x: (-0.93 ,   )  |  t:  +0.00 \n",
      "Time=69:   x: (-0.60 ,   )  |  t:  +0.00 \n"
     ]
    }
   ],
   "source": [
    "def create_addition_data(n_samples=3000):\n",
    "    # This is a dynamic variant of the adding problem. \n",
    "    \n",
    "    # random numbers in [-1.0,1.0]): \n",
    "    X1 = np.random.uniform(low=-1.0, high=1.0, size=(n_samples,) )   \n",
    "    \n",
    "    # random markers [-1.0, 0.0, 1.0] (1.0 marks the numbers that should be added):\n",
    "    X2 = np.random.choice([-1.0,0.0,1.0], size=(n_samples,), p=[0.475,0.475,0.05])\n",
    "    # combine\n",
    "    X = np.vstack((X1, X2)).T.astype('float32')\n",
    "\n",
    "    # create targets\n",
    "    T = np.zeros( (n_samples,1) ).astype('float32')\n",
    "\n",
    "    markers = np.nonzero(X2==1.0)[0]   # get indices of 1.0\n",
    "    \n",
    "    mem = X1[ markers[0] ]\n",
    "    for mi, marker in enumerate(markers[1:]):\n",
    "        T[marker] = mem + X1[ marker ]\n",
    "        mem = X1[ marker ]\n",
    "                \n",
    "    return TupleDataset(X, T)\n",
    "\n",
    "\n",
    "n = 70  # long as the markers x are sparse\n",
    "\n",
    "print \"Example data for the adding problem (x marks 1.0):\"\n",
    "data = create_addition_data(n_samples=n)\n",
    "\n",
    "for t in xrange(n):\n",
    "    print \"Time=%2d:   x: (%+.2f , %s )  |  t:  %+.2f \" % (t, data[t][0][0], \n",
    "                                                        'x' if data[t][0][1] == 1.0 else ' ', \n",
    "                                                        data[t][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialIterator(object):\n",
    "\n",
    "    def __init__(self, data, batch_size=1):\n",
    "        self.data = data\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(self.data) // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        self.idx = -1\n",
    "\n",
    "        offsets = [i * self.n_batches for i in range(self.batch_size)]\n",
    "\n",
    "        # define custom ordering; we won't process beyond the end of the trial\n",
    "        self._order = []\n",
    "        for iter in range(self.n_batches):\n",
    "            x = [(offset + iter) % len(self.data) for offset in offsets]\n",
    "            self._order += x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "        if self.idx == self.n_batches:\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.idx * self.batch_size\n",
    "        \n",
    "        test = list(self.data[self._order[i:(i + self.batch_size)]])\n",
    "\n",
    "        return list(self.data[self._order[i:(i + self.batch_size)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, n_samples=3000, n_epochs=100): \n",
    "    \n",
    "    batchsize = 20\n",
    "    \n",
    "    # Define data iterators\n",
    "    train_iter = SequentialIterator(create_addition_data(n_samples), batch_size=batchsize)\n",
    "    test_iter  = SequentialIterator(create_addition_data(n_samples), batch_size=batchsize)\n",
    "    \n",
    "    cutoff = n_samples / batchsize - 1\n",
    "    cutoff = 70\n",
    "\n",
    "    # Preallocate memory for losses\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "    test_loss = np.zeros(n_epochs)\n",
    "    \n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in tqdm.tqdm(xrange(n_epochs)):\n",
    "\n",
    "        # Training\n",
    "        with chainer.using_config('train', True):\n",
    "\n",
    "            # Reset\n",
    "            model.predictor.reset_state()\n",
    "            counter = 0\n",
    "            loss = None\n",
    "\n",
    "            # Loop over batches\n",
    "            for data in train_iter:\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                # Apply model\n",
    "                _loss = model(data[0], data[1])\n",
    "                train_loss[epoch] += _loss.data\n",
    "\n",
    "                # Gather losses\n",
    "                if loss is None:\n",
    "                    loss = _loss\n",
    "                else:\n",
    "                    loss += _loss\n",
    "\n",
    "                # Update weights\n",
    "                if counter % cutoff == 0:\n",
    "\n",
    "                    # Backprop\n",
    "                    model.cleargrads()\n",
    "                    loss.backward()\n",
    "                    optimizer.update()\n",
    "\n",
    "                    # Resets root of the loss to the current position \n",
    "                    # (chops off computation history):\n",
    "                    loss.unchain_backward()\n",
    "\n",
    "                    # Reset loss\n",
    "                    loss = None\n",
    "\n",
    "        # Normalize loss\n",
    "        train_loss[epoch] /= train_iter.data._length\n",
    "\n",
    "        # Validation\n",
    "        with chainer.using_config('train', False):\n",
    "\n",
    "            model.predictor.reset_state()\n",
    "\n",
    "            # Loop over batches\n",
    "            for data in test_iter:\n",
    "\n",
    "                # Apply model\n",
    "                test_loss[epoch] += model(data[0], data[1]).data\n",
    "\n",
    "        # Normalize loss\n",
    "        test_loss[epoch] /= test_iter.data._length\n",
    "        \n",
    "    return model, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(Chain):\n",
    "\n",
    "    def __init__(self, predictor):\n",
    "        super(Regressor, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.mean_squared_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Testing a network  (2 points)\n",
    "\n",
    "As in previous exercises, we still would like to have a function to test a trained `model`. Assume that `model` is an instance of the `Regressor` class. This function should do the following: \n",
    "\n",
    "1. Generate a new `test` data set, and from it a `test_iter` sequential iterator. It should have `n_samples` data points. \n",
    "1. Let `model.predictor` predict outputs on the test set. For this, iterate through the sequential `test_iter` and pass each `datapoint` through `model.predictor`. \n",
    "1. Save the model output in `pred` and the desired output in `real`. \n",
    "1. For comparison, plot the model output and the the desired output into the same figure. (Add a legend to denote what is what. Plot at least 500 examples.)\n",
    "\n",
    "Your generated data set is a `TupleDataset`. Each data point will thus be a tuple `(x,t)`, where `x` are the inputs and `t` the desired outputs.\n",
    "\n",
    "The recurrent neural network architectures we want to try will be implemented in the next exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test a model\n",
    "def test_network(model, n_samples=3000):\n",
    "    pred = np.zeros(n_samples)\n",
    "    real = np.zeros(n_samples)\n",
    "    \n",
    "    # Generate some test data\n",
    "        ### Add your code here. ###\n",
    "    \n",
    "    # Reset the hidden state before starting to predict:\n",
    "    model.predictor.reset_state()\n",
    "\n",
    "    # Save predicted / desired outputs in pred / real.\n",
    "    for idx, datapoint in enumerate(test_iter):\n",
    "        ### Add your code here. ###\n",
    "        pass\n",
    "\n",
    "    # Plot desired and predicted trace\n",
    "        ### Add your code here. ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Simple RNN  (3 points)\n",
    "\n",
    "We first implement a simple recurrent architecture (a simple [Elman network](http://mnemstudio.org/neural-networks-elman.htm)). \n",
    "\n",
    "1. First implement the linear layers `l1` and `l2`. They should lead from `n_input` input units over `n_hidden` hidden units to `n_out` output units.\n",
    "1. Add a recurrent linear weight layer `hr`. These are weights that self-connect to the hidden units. The input will be the values of the `n_hidden` hidden units, and they should project back to the `n_hidden` hidden units. \n",
    "1. A forward pass will update the hidden state with the inputs and the recurrent layer weights, and produce the output from the hidden unit. Specifically you should do the following: \n",
    "    2. If `self.hstate` contains no value yet (is `None`), it should be set to the input passed through `l1` and `tanh` activations.\n",
    "    2. If `self.hstate` contains a value: a) Pass the input through `l1` and `tanh` activations. b) Pass `self.hstate` through the recurrent weight layer `hr`. The sum of a) and b) should be the new `self.hstate`.\n",
    "    2. Finally pass `self.hstate` through layer `l2`. This produces the output `y`.\n",
    "1. Fill in the function `reset_state()` that resets `self.hstate` back to  `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(Chain):\n",
    "    \n",
    "    def __init__(self, n_hidden, n_input=2, n_out=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            ### Add your code here. ###\n",
    "            pass\n",
    "        \n",
    "        # initialize hidden state with None\n",
    "        self.hstate = None\n",
    "    \n",
    "    def __call__(self):\n",
    "        ### Add your code here. ###\n",
    "        pass\n",
    "    \n",
    "    def reset_state(self):\n",
    "        ### Add your code here. ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Setup and run (1 point)\n",
    "\n",
    "Try your `SimpleRNN` with the dynamic addition task. \n",
    "\n",
    "1. Define the model. `SimpleRNN` should have **4 hidden units** and become the predictor of a `Regressor` instance. \n",
    "1. Train your model for 500 epochs on a dataset of 3000 samples with `train_network`.\n",
    "1. Plot the train and validation losses. \n",
    "1. Finally, use the trained model together with `test_network` (at least 500 examples) to observe how predictions and real data compare. \n",
    "\n",
    "Based on the losses and predictions, what would your conclusion be? Did the simple RNN learn the task? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Train model\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Plot losses\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Plot predictions vs. real data\n",
    "    ### Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: LSTM RNN (2 points)\n",
    "\n",
    "Long-Short Term Memory (LSTM) units have more [powerful functionality](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), such as selective forgetting, and they are able to keep track of long-term dependencies. This might be useful for the adding task. \n",
    "\n",
    "Implement `LSTM_RNN`:\n",
    "\n",
    "1. `lstm` should be an `LSTM` layer leading from the `n_input` inputs to the `n_hidden` hidden units.\n",
    "1. `fc` should be a fully-connected (linear) layer leading from the hidden units (output of `lstm`) to the `n_out` output units. \n",
    "1. The network does not make use of further activation functions. \n",
    "1. Fill in the function `reset_state()` by calling the reset function (with the same name) on the `lstm` layer there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(Chain):\n",
    "    \n",
    "    def __init__(self, n_hidden, n_input=2, n_out=1):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            ### Add your code here. ###\n",
    "            pass\n",
    "    \n",
    "    def __call__(self):\n",
    "            ### Add your code here. ###\n",
    "        pass\n",
    "    \n",
    "    def reset_state(self):\n",
    "            ### Add your code here. ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 5: Setup and run (1 point)\n",
    "\n",
    "Try your `LSTM_RNN` with the dynamic addition task. \n",
    "\n",
    "1. Define the model. `LSTM_RNN` should have **2 hidden units** and become the predictor of a `Regressor` instance.  \n",
    "1. Train your model for 2000 epochs on a dataset of 3000 samples with `train_network`. \n",
    "1. Plot the train and validation losses. \n",
    "1. Finally, use the trained model together with `test_network` (at least 500 examples) to observe how predictions and real data compare. \n",
    "\n",
    "Did the LSTM network capture the task better? Did any of the two capture the task perfectly? Or are the two networks on par? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Train model\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Plot losses\n",
    "    ### Add your code here. ###\n",
    "\n",
    "# Plot predictions vs. real data\n",
    "    ### Add your code here. ###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
