{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "## Multilayer perceptron\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Blackboard before the deadline (see Blackboard).\n",
    "    \n",
    "Learning goals:\n",
    "1. Understand and implement a multi-layer perceptron (MLP) with two weight layers\n",
    "1. Derive and implement backpropagation\n",
    "1. Get familiar with the role of softmax units in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the architecture\n",
    "\n",
    "You are about to implement a multi-layer perceptron (MLP), using backpropagation to learn weights for classifying the 10 MNIST handwritten digits. The input, hidden, and output node layers are connected with two weight layers. The $n_h$ hidden layer nodes use sigmoid activations, and the 10 output layer nodes use softmax activations; learning the one-hot encoding / representation of the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Properties of activation functions (1 point)\n",
    "\n",
    "The activation functions we have discussed so far were nonlinear. This property is actually required for MLPs to work.  Show - with a simple mathematical proof - that with a linear activation function $g(a) = b a$ (where b is some constant, e.g. $b=1$) the forward pass of a neural network with two weight layers could be done (more efficiently) by a neural network with a single weight layer. \n",
    "\n",
    "Hint: The forward pass of a neural network with the weight layers $W^1$ and $W^2$ is $y = g( W^2 g(W^1 x) )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: The sigmoid activation function (0.5 points)\n",
    "\n",
    "The hidden layer units apply the sigmoid function on their linear activations $a$: \n",
    "\n",
    "$$f(a) = \\frac{1}{1+\\exp(-a)}$$\n",
    "\n",
    "To compute backpropagation you will need its derivative again, and you have learned that the sigmoid function derivative has a very simple form. \n",
    "\n",
    "Express $\\frac{\\partial f(a)}{\\partial a} = \\frac{\\partial h}{\\partial a}$ in terms of this simple form, using only the hidden unit output $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus exercise: The softmax cross entropy loss (2 bonus points)\n",
    "\n",
    "Our MLP should do multi-class classification, i.e. be able to classify all 10 digits in MNIST, not just two. A single output unit with a sigmoid unit would be a 2-way output and would not work here. Instead we use as many output units as we have classes - for MNIST classification, the output is then a binary vector of length 10. Each output represents the probability for the associated class given a certain input. \n",
    "\n",
    "This means that in the training set the output unit (vector element) corresponding to the right class has the value 1 (probability 1), and all others are 0. This is a so-called *one-hot encoding* of class labels. Here, a good activation function is the *softmax* activation function, defined as: \n",
    "\n",
    "$$ y_k = p(z_k) = \\frac{\\exp(z_k)}{\\sum_{l=1}^K\\exp(z_l)} $$\n",
    "\n",
    "where $K$ represents the number of output units (= classes), and $z_k$ is the activation going into a single of these output units. With softmax, if you want to classify 10 digits, you define 10 output units and apply softmax over the output of each of them. Then the resulting 10 values will: \n",
    "* sum up to 1. \n",
    "* all be in the range $[0,1]$. \n",
    "\n",
    "These properties make it useful for getting the desired probability distribution as output. The output class predicted in the forward pass could then just be the one with the highest probability. \n",
    "\n",
    "For learning the right weights we again combine this activation function with the *cross-entropy cost function*: \n",
    "\n",
    "$$ L = - \\sum_{l=1}^K t_l \\lg (y_l) $$\n",
    "\n",
    "Note that if we would have 2 classes like in the previous assignment, we would have the same definition $L = -t \\log(y) - (1 - t)\\log(1 - y)$ again, as $t_2 = 1 - t_1$ and $y_2 = 1 - y_1$.\n",
    "\n",
    "\n",
    "Taking the derivative of the cross entropy loss $\\frac{\\partial L}{\\partial z_k}$ for the softmax and a single softmax input / activation $z_k$, we will get: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_k} = \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial z_k} = y_k - t_k$$\n",
    "\n",
    "\n",
    "**Bonus assignments (1 point each)**: \n",
    "1. Show how to derive the softmax activation function. You will need to show this for the two cases $i=j: \\frac{\\partial y_i}{\\partial z_i}$ and $i \\neq j: \\frac{\\partial y_i}{\\partial z_j}$. \n",
    "1. Use your result to derive the cross entropy loss for the softmax function $\\frac{\\partial L}{\\partial z_k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The forward pass (1 point)\n",
    "\n",
    "The inputs of the network $x$ are MNIST images. In the forward pass a single training data example $x$ (a vector of size $m \\times 1$) is weighted by a first weight layer $W^1$ (size $n_h \\times m$). Then this activation $a$ is passed into the sigmoid activation function, producing the hidden layer activation $h$ (a vector of size $n_h \\times 1$). The hidden unit values $h$ are then weighted by a second layer of weights $W^2$ (size $10 \\times n_h$), producing the output unit activation $z$ (a vector of size $10 \\times 1$). Then, for each class $k$ there is an output unit with a softmax activation. \n",
    "\n",
    "Write down the equations for the activations $a$, $h$, $z$ and a single output unit $y_k$.\n",
    "\n",
    "Note that in the implementation you can easily compute all $y_k$ at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3\n",
    "Add $\\LaTeX$ here.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "a &=&  \\\\\n",
    "h &=& \\\\ \n",
    "z &=& \\\\\n",
    "y_k &=&  \\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient of the last layer (1 point)\n",
    "\n",
    "To update the weights so to gradually let the network learn the classification, we need the partial derivatives of the weights. To compute the partial derivatives of the weights $W^2$ in the last layer, we have to propagate from the error function back through the softmax activation function to the weights. \n",
    "\n",
    "$w_{2j}$ are the weights (a row of $W^2$, i.e. size $10 \\times 1$) leading from a single hidden unit $h_j$ to all output units. Obtain $\\frac{\\partial L}{\\partial w_{2j}}$ by applying the chain rule multiple times. $L$ is the cross-entropy loss, and it receives the result of the $K$ softmax output units. We have already provided you with its derivative for the softmax function $\\frac{\\partial L}{\\partial \\mathbf{z}}$ further up. $\\frac{\\partial \\mathbf{z}}{\\partial w_{2j}}$ again has a trivial derivative. \n",
    "\n",
    "Note that in the implementation you should obtain all weight update rows at once with a single matrix multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4:\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Gradient of the first layer (1.5 points)\n",
    "\n",
    "To compute the partial derivatives of the weights $w_1$ in the first layer, we have to propagate from the error function back through the last layer into the first layer (i.e., apply backproagation).  In a similar vein as above, using a sequence of the chain rule, derive the chain of partial derivatives to compute $\\frac{\\partial L}{\\partial w_{1j}}$ (the weights from all input units to a single hidden unit).\n",
    "\n",
    "Due to the 4 intermediate variables $y$, $z$, $h$ and $a$ the chain rule product will have 5 terms here. You should combine $\\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\mathbf{z}}$ to $\\frac{\\partial L}{\\partial \\mathbf{z}}$, which is $\\mathbf{y}-\\mathbf{t}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5:\n",
    "Write $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implementation (3 points)\n",
    "Now that all the math is done, we can start implementing the two-layer network for binary classification of two digits, where we make use of sigmoid units and the cross-entropy loss. Write the following functions:\n",
    "1. `cross_entropy(Y, T)`: Computes the cross entropy loss, averaged over examples `N`. Make sure that there are no exact-zero inputs for `np.log()` (something simple like adding a very small number to `Y` is fine). Note that you sum $L$ over the classes `K` (check the `axis=` parameter of `np.sum`).\n",
    "1. `sigmoid(A)`: Passes the activity matrix `A` through the sigmoid activation function.\n",
    "1. `softmax(A)`: Passes the activity matrix `A` through the softmax activation function. Note that you can compute softmax in one literal line. You just need to sum the denominator over the right dimension. \n",
    "1. `linear(X, W)`: Computes the activities `A` as `X` weighted by `W`.\n",
    "1. `forward(X, W1, W2)`: Computes the forward pass for the two-layer network with sigmoid activations in the first and softmax activations in the second node layer. Returns `Y` and `H`. \n",
    "1. `backward(X, H, Y, W2, T)`: Computes the backward pass for the two-layer network with sigmoid units and cross-entropy loss.\n",
    "1. `train_network(X_train, T_train, X_val, T_val, n_epochs, eta)`: Implements the training procedure (learn the weights). See the skeleton code for some help. Use the fuction `initialize_weights(n_in, n_out)` to initialize your weights with the right shapes.\n",
    "1. `test_network(X, W)`: Predicts class labels for a set of `N` new and unseen training data examples, given as `X`. Interpret the output vector of length `10` as a set of probabilities for the class labels `0, 1, 2, 3, 4, 5, 6, 7, 8, 9` (Integer values). For each of the `N` output vectors, return the class label with the highest probability. \n",
    "\n",
    "Last time we initialized the weights from a Gaussian normal distribution. This time we initialize them by drawing uniformly from the rule of thumb range $ \\left [ - \\frac{ \\sqrt{6} }{ \\sqrt{n + m} },  \\frac{ \\sqrt{6} }{ \\sqrt{ n + m} } \\right ] $ ($n\\times m$ being the weight matrix dimensions), which  works better here. Weight initialization can have quite some influence on your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n_in, n_out):\n",
    "    \"\"\"\n",
    "    Initializes a weight matrix.\n",
    "    INPUT:\n",
    "        n_in  = 1 number of input units.\n",
    "        n_out = 1 number of output units\n",
    "    OUTPUTS\n",
    "        W = [n_out n_in] the initial weight matrix\n",
    "    \"\"\"\n",
    "    r = np.sqrt(6) / np.sqrt(n_out + n_in)\n",
    "    return np.random.uniform(-r, r, [n_out, n_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(Y, T):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "    INPUT:\n",
    "        Y = [K N] output vector for N examples and K units (classes)\n",
    "        T = [K N] target vector for N examples and K units (classes)\n",
    "    OUTPUTS\n",
    "        L = 1  the mean cross-entropy loss\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes the softmax activation function. \n",
    "    INPUT:\n",
    "        Z = [10 N] vector of input activations for 10 output units and N examples\n",
    "    OUTPUTS\n",
    "        Y = [10 N] the vectors of softmax activations for 10 output units and N examples\n",
    "    \"\"\"\n",
    "    Z -= np.max(Z, axis=0)  # for numerical stability\n",
    "    ### Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(A):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "    INPUT:\n",
    "        A = [H N] activity matrix of H units for N examples\n",
    "    OUTPUT\n",
    "        Y = [H N] output matrix of H units for N examples\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    \"\"\"\n",
    "    Computes the activities for a fully connected layer.\n",
    "    INPUT:\n",
    "        X = [P N] data matrix of P input units for N examples\n",
    "        W = [Q P] weight matrix of P inputs to Q outputs\n",
    "    OUTPUT\n",
    "        A = [Q N] activity matrix of Q output units for N examples\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, W2):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a two-layer network with sigmoid units.\n",
    "    INPUT\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        W1 = [Q  P] weight matrix of the first layer of P inputs to Q outputs\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "    OUTPUT\n",
    "        H = [Q  N] output matrix of Q hidden units for N examples\n",
    "        Y = [10 N] output vector for N examples\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return H, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(X, H, Y, W2, T):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a two-layer network with sigmoid and softmax units, and cross-entropy loss.  \n",
    "    INPUT:\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        H  = [Q  N] output matrix of Q hidden units for N examples\n",
    "        Y  = [10 N] output probability vectors for N examples\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "        T  = [10 N] a vector of one-hot encoded targets for N examples\n",
    "    OUTPUT\n",
    "        dW1 = [Q P] gradient matrix for the weights of layer 1 of P inputs to Q outputs\n",
    "        dW2 = [10 Q] gradient matrix for the weights of layer 2 of Q inputs to 10 outputs\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(X_train, T_train, X_val, T_val, n_hidden=30, n_epochs=300, eta=0.001):\n",
    "    \"\"\"\n",
    "    Performs the training procedure for a two-layer network with sigmoid and softmax units, and cross-entropy loss. \n",
    "    INPUT:\n",
    "        X_train  = [P  N] data matrix of P inputs for N training examples\n",
    "        T_train  = [10 N] a vector of targets for N training examples\n",
    "        X_val    = [P  M] data matrix of P inputs for N training examples\n",
    "        T_val    = [10 M] a vector of targets for N training examples\n",
    "        n_hidden = 1  number of hidden units (default 10)\n",
    "        n_epochs = 1  number of training epochs (default 100)\n",
    "        eta      = 1  learning rate (default 0.001)\n",
    "    OUTPUT:\n",
    "        W1         = [Q  P] the learned weights for layer 1 of P inputs to Q outputs\n",
    "        W2         = [10 Q] the learned weights for layer 2 of Q inputs to 10 outputs\n",
    "        train_loss = [Z 1] the training loss for Z epochs\n",
    "        val_loss   = [Z 1] the validation loss for Z epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize W1 and W2 (use initialize_weights())\n",
    "    ### Add your code here. ###\n",
    "    \n",
    "    # Loop over epochs\n",
    "    train_loss = np.zeros((n_epochs))\n",
    "    val_loss = np.zeros((n_epochs))\n",
    "    for i_epoch in xrange(n_epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        ### Add your code here. ###\n",
    "        \n",
    "        # Backward pass\n",
    "        ### Add your code here. ###\n",
    "        \n",
    "        # Parameter update\n",
    "        ### Add your code here. ###\n",
    "        \n",
    "        # Save loss\n",
    "        ### Add your code here. ###\n",
    "        \n",
    "        # Print progress and loss\n",
    "        if i_epoch % 10 == 0:\n",
    "            print(\"Epoch {}/{}. Train loss: {}. Validation loss: {}.\".format(\n",
    "                1+i_epoch, n_epochs, train_loss[i_epoch], val_loss[i_epoch]))\n",
    "        \n",
    "    return W1, W2, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_network(X, W1, W2):\n",
    "    \"\"\"\n",
    "    Applies the trained two-layer network with sigmoid units to data.\n",
    "    INPUT:\n",
    "        X  = [P  N] data matrix of P inputs for N examples\n",
    "        W1 = [Q  P] weight matrix of the first layer of P inputs to Q outputs\n",
    "        W2 = [10 Q] weight vector of the second layer of Q inputs to 10 outputs\n",
    "    OUTPUT\n",
    "        classes = [1 N] prediction vector (i.e., predicted integer labels from 0 to 9) for N examples\n",
    "    \"\"\"\n",
    "    ### Add your code here. ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "Below we first load in (a subset of) the MNIST handwritten digit dataset, and restrict it to two digits. We plot some examples. We split this data into a training and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read full dataset from mat file\n",
    "mat = sio.loadmat(\"digits.mat\")\n",
    "\n",
    "# The data set contains 1000 examples of each class in sequence - create the corresponding label vector: \n",
    "T = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).repeat(1000)\n",
    "\n",
    "# The 1000 examples of the 0-class are currently at the end of the data set. \n",
    "# Move them to the beginning. Then we can use np.argmax to get from one-hot encoded class probabilites to\n",
    "# the original class label: \n",
    "X = np.roll(mat[\"digits\"], shift=1000, axis=1)\n",
    "\n",
    "sz = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot some examples\n",
    "fig, ax = plt.subplots(1, 10)\n",
    "fig.set_size_inches([15, 7])\n",
    "for i in xrange(10):\n",
    "    ax[i].imshow(X[:, 0 + i*1000].reshape(sz).T, cmap=\"gray\")\n",
    "    ax[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in training, validation, and testing split\n",
    "X_train, X_test, T_train, T_test = train_test_split(X.T, T, test_size=0.2)\n",
    "X_train, X_val, T_train, T_val = train_test_split(X_train, T_train, test_size=0.2)\n",
    "\n",
    "# Transpose back\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# Transform the label sets used in training to one-hot vectors: \n",
    "T_train = np.eye(10)[T_train].T\n",
    "T_val = np.eye(10)[T_val].T\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions training inputs: {}, and training outputs: {}\".format(X_train.shape, T_train.shape))\n",
    "print(\"Dimensions validation inputs: {}, and validation outputs: {}\".format(X_val.shape, T_val.shape))\n",
    "print(\"Dimensions testing inputs: {}, and testing outputs: {}\".format(X_test.shape, T_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Training (1 point)\n",
    "Now that we have done all the work, we can finally run the network. Below we first load in the digit dataset, and restrict it to two digits. We split this data into a training and a test set. \n",
    "\n",
    "1. Train your network on the training dataset `X_train` and `T_train`, and validate it at each epoch on the test set `X_val` and `T_val`. Use $n_h=30$ hidden units, 300 epochs and a learning rate of $\\eta=0.001$ (default values).\n",
    "1. After training, plot the train and validation losses over epochs (as returned by `train_network()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train network\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Plot losses\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Testing (1 point)\n",
    "Now that the network is trained, we can obtain a test score on a held out test set, and compute a classification performance. Apply your network to the test set `X_test` and `T_test`, and print its accuracy. \n",
    "\n",
    "If everything went fine, the accuracy should be above 90%, which is fine as we only use 1/6 of the original MNIST data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test network\n",
    "###  Add your code here. ###\n",
    "\n",
    "# Print accuracy\n",
    "###  Add your code here. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has the network learned?\n",
    "\n",
    "What has the MLP learned to be able to classify the digits with relatively high accuracy? We can easily inspect a part of the network, that is the weights coming directly after the input nodes (i.e. the first layer weights) to check which patterns the MLP deemed important for correct classification at this stage (for higher layers this inspection is more complicated, and an active research field). \n",
    "\n",
    "For this we just need to reshape the first layer weights leading from all input values to a hidden unit to $28 \\times 28$. We do this separately for each hidden unit to check what each of them represents or detects. These patterns act a bit like *receptive fields*.\n",
    "\n",
    "When you have trained the full network, just run the code in the next cell to check the learned pattern detectors in the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhidden = w1.shape[0]\n",
    "\n",
    "plt.figure(figsize=([8,10]))\n",
    "for i in range(nhidden):    \n",
    "    plt.subplot(5,6,i+1)\n",
    "    fig = plt.imshow(w1[i, :].reshape([28,28]).T, cmap='gray')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
